{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursework 1: Gradient Descent (27 Points)\n",
    "### Autograding\n",
    "Part 1 of this coursework is autograded. This notebook comes with embedded tests which will verify that your implementations provide outputs with the appropriate types and shapes required for our hidden tests. You can run these same public tests through [LabTS](https://teaching.doc.ic.ac.uk/labts) when you have finished your work, to check that we get the same results when running these public tests.\n",
    "\n",
    "Hidden tests will be ran after the submission deadline, and cannot be accessed :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q otter-grader numpy pandoc seaborn autograd memory-profiler graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization Cell\n",
    "%matplotlib inline\n",
    "import otter\n",
    "grader = otter.Notebook(\"mml_cw_1.ipynb\")\n",
    "import matplotlib.pyplot as plt # DO NOT use %matplotlib inline in the notebook\n",
    "import numpy as np\n",
    "rng_seed = 90"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1 - Differentiation & Gradient Descent (16 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this question, we define the following constants:\n",
    "\n",
    "$$\\boldsymbol{B}=\\left(\\begin{array}{cc}\n",
    "4 & -2 \\\\\n",
    "-2 & 4\n",
    "\\end{array}\\right), \\quad \\boldsymbol{a}=\\left(\\begin{array}{l}\n",
    "0 \\\\\n",
    "1\n",
    "\\end{array}\\right), \\quad \\boldsymbol{b}=\\left(\\begin{array}{c}\n",
    "-2 \\\\\n",
    "1\n",
    "\\end{array}\\right)$$\n",
    "\n",
    "We define also the following functions, which are all $\\mathbb{R}^2 \\rightarrow \\mathbb{R}$\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&f_1(\\boldsymbol{x})=\\boldsymbol{x}^T \\boldsymbol{B} \\boldsymbol{x}-\\boldsymbol{x}^T \\boldsymbol{x}+\\boldsymbol{a}^T \\boldsymbol{x}-\\boldsymbol{b}^T \\boldsymbol{x} \\\\\n",
    "&f_2(\\boldsymbol{x})=\\cos \\left((\\boldsymbol{x}-\\boldsymbol{b})^T(\\boldsymbol{x}-\\boldsymbol{b})\\right)+(\\boldsymbol{x}-\\boldsymbol{a})^T \\boldsymbol{B}(\\boldsymbol{x}-\\boldsymbol{a}) \\\\\n",
    "&f_3(\\boldsymbol{x})=1-\\left(\\exp \\left(-(\\boldsymbol{x}-\\boldsymbol{a})^T(\\boldsymbol{x}-\\boldsymbol{a})\\right)+\\exp \\left(-(\\boldsymbol{x}-\\boldsymbol{b})^T \\boldsymbol{B}(\\boldsymbol{x}-\\boldsymbol{b})\\right)-\\frac{1}{10} \\log \\left|\\frac{1}{100} \\boldsymbol{I}+\\boldsymbol{x} \\boldsymbol{x}^T\\right|\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "Implementations of these functions are provided below.\n",
    "\n",
    "Throughout this exercise, we remain consistent in our convention of using row vectors for our gradients $\\left( \\textnormal{i.e. } \\frac{\\partial f_1}{\\partial x}, \\frac{\\partial f_2}{\\partial x}, \\frac{\\partial f_3}{\\partial x} \\in \\mathbb{R}^{1 \\times 2} \\right )$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defined constants\n",
    "B = np.array([[4, -2], [-2, 4]])\n",
    "a = np.array([[0], [1]])\n",
    "b = np.array([[-2], [1]])\n",
    "\n",
    "def f1(x):\n",
    "    \"\"\" Function f1 taking input x with shape (2, 1) \"\"\"\n",
    "    return float(x.T @ B @ x - x.T @ x + a.T @ x - b.T @ x)\n",
    "\n",
    "def f2(x):\n",
    "    \"\"\" Function f2 taking input x with shape (2, 1) \"\"\"\n",
    "    return float(np.cos((x - b).T @ (x - b)) + (x - a).T @ B @ (x - a))\n",
    "\n",
    "def f3(x):\n",
    "    \"\"\" Function f3 taking input x with shape (2, 1) \"\"\"\n",
    "    return float(1 - (np.exp(-(x - a).T @ (x - a)) + \\\n",
    "                 np.exp(-(x - b).T @ B @ (x - b)) - \\\n",
    "                 (1/10.) * np.log(np.linalg.det((1/100.) * np.identity(2) + x @ x.T))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Question 1 - Checking for the existence of Minima (2 Points)\n",
    "Complete the function ```f1_check_minimum(B, a, b)``` that checks whether function $f_1$ has a minimum given certain values of **a**, **b** and diagonal **B**.\n",
    "\n",
    "Hint: you may not need to use all three gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def f1_check_minimum(B, a, b):\n",
    "    \"\"\" Write a function that returns True if function f1 has a minimum for variables B, a and b, and returns False otherwise.\n",
    "        Hint: it may not be required to use all B, a and b. \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    ...\n",
    "    check = ...\n",
    "    return check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"1. Function 1 Minima Check\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Question 2 - Calculating Gradients (6 Points)\n",
    "#### Question 2.a - Method of Finite Differences (2 Points)\n",
    "Remember (animation in lectures) that a gradient is found by taking\n",
    "$$ \\lim _{\\Delta x \\rightarrow 0} \\frac{f(x+\\Delta x)-f(x)}{\\Delta x} $$\n",
    "We can approximate this by calculating the expression for a small but finite $\\Delta x$ along each dimension, which\n",
    "is known as the _finite-differences_ approximation.\n",
    "\n",
    "Complete the function ```grad_fn(fn, x)``` such that it returns the gradients for any function ```fn``` at a point **x** using the method of finite differences. Use a delta of $1\\times 10^{-5}$.\n",
    "\n",
    "_The function should take a columnar numpy (2, 1) vector for ‘x’ as input, and output a\n",
    "numpy (1, 2) row vector for the gradient._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def grad_fd(fn, x, delta=1e-5):\n",
    "    \"\"\" General function that calculates gradient of some 2d function at point x,\n",
    "        using finite-differences.\n",
    "\n",
    "    Inputs:\n",
    "            fn: Function taking input x and returns a scalar\n",
    "            x: Numpy vector of shape (2, 1)\n",
    "            delta: Finite-difference delta (epsilon) used for approximation\n",
    "\n",
    "    Returns: Approximated gradient at point x, in shape (1, 2)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    ...\n",
    "    dfdx = ...\n",
    "    return dfdx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"2.a Method of Finite Differences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Question 2.b - Analytical Gradients (4 Points)\n",
    "Complete the functions ```f1_grad(x)```, ```f2_grad(x)``` and ```f3_grad(x)``` that return\n",
    "gradients of f1, f2 and f3, using your own derivations.\n",
    "\n",
    "_The functions should take a columnar numpy (2, 1) vector for **x** as input, and output a\n",
    "numpy (1, 2) row vector for the gradient_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def f1_grad_exact(x):\n",
    "    \"\"\" Return gradient of f1, exactly derived by hand \"\"\"\n",
    "    # YOUR ANSWER HERE\n",
    "    gradient = ...\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"Q2.b.i Gradients of the Functions - f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def f2_grad_exact(x):\n",
    "    \"\"\" Return gradient of f2, exactly derived by hand \"\"\"\n",
    "    # YOUR ANSWER HERE\n",
    "    gradient = ...\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"Q2.b.ii Gradients of the Functions - f2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def f3_grad_exact(x):\n",
    "    \"\"\" Return gradient of f3, exactly derived by hand \"\"\"\n",
    "    ...\n",
    "    gradient = ...\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"Q2.b.iii Gradients of the Functions - f3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Question 3 - Gradient Descent (8 Points)\n",
    "Use your gradients to implement a gradient descent algorithm with 50 iterations\n",
    "to find a local minimum for both f2 and f3, by finishing the function grad descent(fn,\n",
    "grad fn).\n",
    "\n",
    "For visualizing (and debugging) your gradient descent function, we provide some plotting code. This is contained in the cell below, so be sure to exectue it. You can use this function on f1 by passing in the other functions, for example: ```plot_grad_descent(f1, f1_grad_exact, gradient_descent)``` once you have completed the ```gradient_descent(fn, grad_fn)``` function. You can also pass in ```xrange=(x_min, x_max)``` and likewise for ```yrange``` to adjust the plotted region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide function for plotting gradient descent\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_grad_descent(fn, fn_grad, gradient_descent_fn, xrange=(-1, 1), yrange=(-1,1), **kwargs):\n",
    "    title = 'Plotting function #'+ fn_grad.__name__.split('_')[0][-1]\n",
    "    # Define plotting range for x- and y- axis.\n",
    "    x1min, x1max = xrange\n",
    "    x2min, x2max = yrange\n",
    "\n",
    "    # Evaluate function everywhere within the defined range for the contour plot\n",
    "    x1 = np.linspace(x1min, x1max, 100)\n",
    "    x2 = np.linspace(x2min, x2max, 100)\n",
    "\n",
    "    X1, X2 = np.meshgrid(x1, x2)\n",
    "\n",
    "    Y = [fn(np.array([[p1], [p2]])) for p1, p2 in zip(X1.flatten(), X2.flatten())]\n",
    "    Y = np.array(Y).reshape(X1.shape)\n",
    "\n",
    "    # Plot contour\n",
    "    plt.title(title)\n",
    "    plt.xlim(x1min, x1max)\n",
    "    plt.ylim(x2min, x2max)\n",
    "    plt.contourf(X1, X2, Y)\n",
    "    plt.colorbar()\n",
    "\n",
    "    # Plot gradient descent trajectory\n",
    "    trajectory, found_minimum, found_minimum_value = gradient_descent_fn(fn, fn_grad, **kwargs)\n",
    "\n",
    "    p1, p2 = zip(*trajectory)\n",
    "    plt.plot(p1, p2, '.-', color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3.a - Implementing Gradient Descent (4 Points)\n",
    "Complete the ```gradient_descent``` function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gradient_descent(fn, grad_fn, start_x=3.0, start_y=3.0, lr=0.001, n_steps=10, silent=False):\n",
    "    \"\"\" Function that performs gradient descent.\n",
    "\n",
    "    Inputs: \n",
    "        - fn: Function to minimize\n",
    "        - grad_fn: Function that returns gradient of the function to minimize\n",
    "        - start_loc: Initial location\n",
    "        - lr: The learning rate\n",
    "        - n_steps: Number of steps\n",
    "        - silent: prevent print statement (for testing)\n",
    "\n",
    "    Returns: Tuple containing:\n",
    "        - trajectory of found points: a list containing numpy (2, 1) column vectors\n",
    "        - final minimum point: a numpy (2, 1) column vector\n",
    "        - the value at the minimum: float\n",
    "    \"\"\"\n",
    "\n",
    "    start_loc = np.array([[start_x], [start_y]])\n",
    "    trajectory = [start_loc]\n",
    "\n",
    "    ...\n",
    "\n",
    "    found_minimum_loc = ...\n",
    "    found_minimum_value = ...\n",
    "    # if not silent:\n",
    "    #     print(f\"Gradient descent found minimum value {found_minimum_value:.2f} at {found_minimum_loc.T}^T\")\n",
    "    return trajectory, found_minimum_loc, found_minimum_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Here we'll use the plotting function and specify custom starting points and plotting ranges\n",
    "plot_grad_descent(f1, f1_grad_exact, gradient_descent,\n",
    "                  xrange=(2, 3), yrange=(2, 4),\n",
    "                  start_x=3, start_y=3, lr=0.005\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** The last two tests should be run after the first one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"Gradient Descent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3.b - Choosing Good Initializations (2 Points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Here we'll use the plotting function and specify custom starting points and plotting ranges\n",
    "# For F2 \n",
    "f2_start_x = ...\n",
    "f2_start_y = ...\n",
    "f2_lr = ...\n",
    "f2_n_steps = ...\n",
    "\n",
    "plot_grad_descent(f2, f2_grad_exact, gradient_descent,\n",
    "                  xrange=(-1.5, 1.5), yrange=(-1.5, 1.5),\n",
    "                  start_x=f2_start_x, start_y=f2_start_y, lr=f2_lr, n_steps=f2_n_steps, silent=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For F3 \n",
    "f3_start_x = ...\n",
    "f3_start_y = ...\n",
    "f3_lr = ...\n",
    "f3_n_steps = ...\n",
    "\n",
    "plot_grad_descent(f3, f3_grad_exact, gradient_descent,\n",
    "                  xrange=(-1.5, 1.5), yrange=(-1.5, 1.5),\n",
    "                  start_x=f3_start_x, start_y=f3_start_y, lr=f3_lr, n_steps=f3_n_steps, silent=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"Gradient Descent Initialization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3.c - Failure to Converge (2 Points)\n",
    "Find an example of diverging behaviour and describe three ways that we can encourage convergence in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of diverging behaviour\n",
    "# plot_grad_descent(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "ANSWER HERE - you "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_grad_descent(f2, f2_grad_exact, gradient_descent, lr=0.4, n_steps=50, silent=True, xrange=(-20, 20), yrange=(-20, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "---\n",
    "## Part 2 - Automatic Differentation (11 Points)\n",
    "\n",
    "In the previous part you saw how you can implement gradient descent to optimize functions by using either approximate or analytical gradients. Luckily for us, there are framework which implement derivatives for most functions we might typically care about, and which allow us to differentiate through arbitrary compositions of these functions by using **Automatic Differentiation** (autodiff). Indeed, you may already have encountered frameworks such as PyTorch, TensorFlow or Jax which have robust autodiff implementations.\n",
    "\n",
    "In this part we'll demonstrate the power of autodiff through the [autograd](https://github.com/HIPS/autograd) library which provides a clean interface for automatically differentiating over numpy functions. Let's start by taking a look at the syntax for computing gradients in autograd:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autograd import grad \n",
    "import autograd.numpy as np # Autograd wraps numpy to support automatic differentiation\n",
    "# Note that particularly niche numpy functions may not be supported by autograd\n",
    "\n",
    "# Consider the sum of squares function\n",
    "def xsq(x): \n",
    "    return np.sum(x**2)\n",
    "\n",
    "# We know the gradient of this! \n",
    "def our_grad_of_xsq(x):\n",
    "    return 2*x\n",
    "\n",
    "x = np.random.rand(4)\n",
    "\n",
    "# We can wrap functions with grad to get the gradient function\n",
    "autograd_grad_of_xsq = grad(xsq, 0) # 0 => want the gradient with respect to the first argument\n",
    "\n",
    "# Evaluate gradients at x and check that they're the same\n",
    "print(autograd_grad_of_xsq(x))\n",
    "print(our_grad_of_xsq(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Question 4 - Squared Distances and Automatic Differentiation\n",
    "Although we can see that autograd is perfectly capable of computing derivatives by itself, there may be instances where the derivative of a non-elementary function is analytically *more efficient* than the composition of the gradients of its components. This means that we are better off telling autograd to use our derivate when it performs automatic differentation (sometimes we might also use esoteric functions which don't even have an existing derivate, in which case we are forced to provide one).\n",
    "\n",
    "#### Question 4.a - Forward Pass of the Squared Distances function (1 Point)\n",
    "In particular, we'll consider the gradient of \n",
    "$$ \\vec{s} = \\vec{p}^T \\mathbf{D} \\vec{q}, $$\n",
    "with respect to $\\vec{z}$, where $\\vec{p}$ and $\\vec{q}$ are constants, and $\\mathbf{D}$ is the matrix of squared distances between elements of $\\vec{z}$:\n",
    "$$ D_{ij} = \\exp(-(z_i - z_j)^2). $$\n",
    "\n",
    "Get warmed up by implementing the function ```sq_dist_fwd(p,q,x)``` which computes $\\vec{s}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "np.random.seed(0)\n",
    "\n",
    "# Naive solution\n",
    "p = np.random.rand(4)\n",
    "q = np.random.rand(4)\n",
    "def sq_dist_fwd(p,q,x):\n",
    "    \"\"\" Compute the inner product of p and q, projected through a matrix consisting of the \n",
    "        squared distance between elements of a variable x. All vectors are length x.shape[0].\n",
    "        To be compatible with autograd, we need to avoid using assignments on array indices \n",
    "    \"\"\"\n",
    "    \n",
    "    N = x.shape[0]\n",
    "    assert N % 2 == 0 # We expect even numbers of elements\n",
    "    assert x.shape[0] == p.shape[0] == q.shape[0]\n",
    "    ...\n",
    "    out = ...\n",
    "    return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"Q4.a Squared Distance Function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4.b - Analytical Derivate of Squared Distances (2 Points)\n",
    "In order to override autograd's gradient for our squared distance function, we need to derive it ourselves - compute the derivate\n",
    "$$\\frac{\\partial \\vec{s}}{\\partial \\vec{z}} =\\sum_{i j} a_i b_j \\frac{\\partial D_{i j}}{\\partial \\vec{z}} $$\n",
    "with respect to one element of $\\vec{z}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "#### Question 4.c - Implement the Derivative (2 Points)\n",
    "With the analytical derivate in hand, complete the ```sq_dist_grad_elem(a, b, x, z)``` function which computes the gradient with respect to the $n^{th}$ dimension of $\\vec{z}$ and the corresponding function ```sq_dist_grad``` which computes the gradient vector.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Autograd solution\n",
    "def sq_dist_grad_elem(p, q, z, n):\n",
    "    ...\n",
    "\n",
    "def sq_dist_grad(p, q, z):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"Q4.c Squared Distance Function Gradients\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4.d - Automatic Differentiation (2 Points)\n",
    "\n",
    "Armed with our superior gradient function for the sum of squared distances, let's tell autograd to use it, and convince ourselves that all our hardwork was worthwhile!\n",
    "\n",
    "First, let's compare memory usage to check whether our implementation actually requires less memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler\n",
    "p,q,z = np.random.randn(200), np.random.randn(200), np.random.randn(200)\n",
    "print('Ours: ', end=' ')\n",
    "%memit sq_dist_grad(p,q,z)\n",
    "print('Theirs:', end=' ')\n",
    "%memit grad(sq_dist_fwd, 2)(p,q,z) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that our implementation of the gradient function requires considerably less memory than the one autograd provides by using autodiff. \n",
    "\n",
    "Now let's tell autograd to use our analytically derived gradient function when computing autodiff for a more complicated function:\n",
    "$$ \\vec{y} = \\vec{s} = \\vec{p}^T \\mathbf{D}(\\vec{z}) \\vec{q}, \\quad \\text{and} \\quad \\vec{z} = 3\\sin{x}+5$$\n",
    "\n",
    "We'll start by revisiting our toy example of $x^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autograd.extend import primitive, defvjp\n",
    "# First we'll consider the x squared example again\n",
    "\n",
    "@primitive # This tells autograd we will define the gradient ourselves\n",
    "def xsq(x): \n",
    "    return np.sum(x**2)\n",
    "\n",
    "# Here's an example of how to define a custom backward pass gradient function\n",
    "def xsq_vjp(ans, x):\n",
    "    \"\"\" This should return a function which takes the gradient (g) of the SUBSEQUENT function\n",
    "        and combines it with the gradient of the function we're differentiating \n",
    "        (i.e. propagating the accumulated gradient backwards through the graph)\n",
    "    \"\"\"\n",
    "    print('We are constructing the computational graph')\n",
    "\n",
    "    def xsq_vjp_inner(g):\n",
    "        print('We are auto differentiating')\n",
    "        print(f'gradient in was {g}, answer in was {ans}\\n')\n",
    "        return 2*x*g\n",
    "    \n",
    "    print('The gradient function has been stored in the graph \\n')\n",
    "    return xsq_vjp_inner\n",
    "\n",
    "defvjp(xsq, xsq_vjp)\n",
    "\n",
    "print('Gradient out: ', grad(xsq)(np.array([1., 2., 3.])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's apply the same idea to our squared distance function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from functools import partial \n",
    "# We'll overwrite the naive solution with a primitive\n",
    "# to make life easier let's fix p and q with \"partial\"\n",
    "p,q,z = np.random.randn(4), np.random.randn(4), np.random.randn(4)\n",
    "sq_dist_fwd_wrapped = partial(sq_dist_fwd, p, q)\n",
    "sq_dist_grad_wrapped = partial(sq_dist_grad, p, q)\n",
    "\n",
    "@partial \n",
    "def sq_dist(z):\n",
    "    # Tell autograd we a custom gradient for sq_dist\n",
    "    ...\n",
    "\n",
    "def sq_dist_vjp(ans, z): \n",
    "    # Return a function which takes the gradient (g) of the SUBSEQUENT function\n",
    "    # and combines it with the gradient of sq_dist\n",
    "    ...\n",
    "\n",
    "defvjp(sq_dist, sq_dist_vjp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"Q4.d Squared Distance Autodiff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider the function\n",
    "x = np.random.rand(4)\n",
    "z = np.sin(x)*3 + 5\n",
    "y = sq_dist(z)\n",
    "\n",
    "# we can compute the gradient of y with respect to x using autograd\n",
    "def complete_function(x):\n",
    "    z = np.sin(x)*3 + 5\n",
    "    return sq_dist(z) # This will now use our gradient function!\n",
    "\n",
    "grad(complete_function)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4.e - Understanding Autodiff (4 Points)\n",
    "Now that we've seen how to define custom gradients for autodiff, let's double check our understanding:\n",
    "1. Draw the computational graph for the ```complete_function(x)``` above\n",
    "2. Estimate the Memory and Time complexity of the autodiff gradient calculation, and compare this to ours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Question 4.e.i - Create a (rough) computational graph for the function\"\"\"\n",
    "\n",
    "from graphviz import Digraph\n",
    "comp_graph = Digraph('Computational Graph') # Create Digraph object\n",
    "\n",
    "with comp_graph.subgraph(name='cluster_0') as c:\n",
    "    c.attr(style='filled', color='pink', label='First Function')\n",
    "    c.node_attr.update(style='filled', color='white')\n",
    "    c.edges([('x', 'sin(x)')])\n",
    "\n",
    "with comp_graph.subgraph(name='cluster_1') as c:\n",
    "    c.attr(style='filled', color='lightblue', label='Squared Distance Function')\n",
    "    c.node_attr.update(style='filled', color='white')\n",
    "    c.edges([('placeholder1', 'placeholder2')])\n",
    "    \n",
    "comp_graph.node('z')\n",
    "comp_graph.edge('sin(x)', 'z')\n",
    "comp_graph.edge('z', 'placeholder1')\n",
    "\n",
    "comp_graph.attr(label=r'\\n\\nComputational Graph for Q4')\n",
    "comp_graph.attr(fontsize='20')\n",
    "comp_graph "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.e.ii - Complexity Answer:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Question 4.e.i - Create a (rough) computational graph for the function\"\"\"\n",
    "\n",
    "from graphviz import Digraph\n",
    "comp_graph = Digraph('Computational Graph') # Create Digraph object\n",
    "\n",
    "with comp_graph.subgraph(name='cluster_0') as c:\n",
    "    c.attr(style='filled', color='pink', label='First Function')\n",
    "    c.node_attr.update(style='filled', color='white')\n",
    "    c.edges([('x', 'sin(x)')])\n",
    "\n",
    "with comp_graph.subgraph(name='cluster_1') as c:\n",
    "    c.attr(style='filled', color='lightblue', label='Squared Distance Function')\n",
    "    c.node_attr.update(style='filled', color='white')\n",
    "    c.edges([('placeholder1', 'placeholder2')])\n",
    "    \n",
    "comp_graph.node('z')\n",
    "comp_graph.edge('sin(x)', 'z')\n",
    "comp_graph.edge('z', 'placeholder1')\n",
    "\n",
    "comp_graph.attr(label=r'\\n\\nComputational Graph ANSWER for Q4')\n",
    "comp_graph.attr(fontsize='20')\n",
    "comp_graph "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "---\n",
    "## Submission\n",
    "Once you have completed all questions and re-ran all tests, simply push your final notebook to gitlab. You'll want to go to the [LabTS Exercise](https://teaching.doc.ic.ac.uk/labts/lab_exercises/2223/exercises/732/exercise_summary) and check that there are no strange bugs on our end - note that the test results you see on LabTS should be identical to the tests provided in this notebook; of course, we also have other tests which will be run for marking purposes :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "To double-check your work, the cell below will rerun all of the autograder tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mml_cw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 (default, Mar 28 2022, 06:13:39) \n[Clang 12.0.0 ]"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "1. Function 1 Minima Check": {
     "name": "1. Function 1 Minima Check",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> # Check with minimum\n>>> f1_check_minimum(B, a, b)\nTrue",
         "failure_message": "F1 Minimum Check (with minimum) Test Failed",
         "hidden": false,
         "locked": false,
         "points": 0,
         "success_message": "F1 Minimum Check (with minimum) Test Passed"
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "2.a Method of Finite Differences": {
     "name": "2.a Method of Finite Differences",
     "points": 4,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> B = np.array([[4, -2], [-2, 4]])\n>>> a = np.array([[0], [1]])\n>>> b = np.array([[-2], [1]])\n>>> def f1(x):\n...     return float(x.T @ B @ x - x.T @ x + a.T @ x - b.T @ x)\n>>> dummy_input = np.array([[2.5], [3.5]])\n>>> output = grad_fd(f1, dummy_input)\n>>> \n>>> target_output = np.array([[3.00003, 11.00003]])\n>>> np.isclose(output, target_output, atol=1e-3).all()\nTrue",
         "failure_message": "Finite Differences on f1 Test Failed",
         "hidden": false,
         "locked": false,
         "points": 1,
         "success_message": "Finite Differences on f1 Test Passed"
        },
        {
         "code": ">>> B = np.array([[4, -2], [-2, 4]])\n>>> a = np.array([[0], [1]])\n>>> b = np.array([[-2], [1]])\n>>> def f2(x):\n...     return float(np.cos((x - b).T @ (x - b)) + (x - a).T @ B @ (x - a))\n>>> dummy_input = np.array([[2.5], [3.5]])\n>>> output = grad_fd(f2, dummy_input)\n>>> \n>>> target_output = np.array([[1.18572957, 5.10321673]])\n>>> np.isclose(output, target_output, atol=1e-3).all()\nTrue",
         "failure_message": "Finite Differences on f2 Test Failed",
         "hidden": false,
         "locked": false,
         "points": 0,
         "success_message": "Finite Differences on f2 Test Passed"
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "Gradient Descent": {
     "name": "Gradient Descent",
     "points": 4,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> c = np.array([[4], [5]])\n>>> dummy_fn = lambda x: float(c.T @ x)\n>>> dummy_fn_grad = lambda x: c.T\n>>> dummy_start_x, dummy_start_y = 1.0, 2.0\n>>> \n>>> trajectory, minimum_loc, minimum_value = gradient_descent(dummy_fn, dummy_fn_grad, start_x=dummy_start_x, start_y=dummy_start_y, lr=0.01, n_steps=5, silent=True)\n>>> \n>>> target_trajectory = [np.array([[1], [2]]), np.array([[0.96], [1.95]]), np.array([[0.92], [1.9]]),\n...                     np.array([[0.88], [1.85]]), np.array([[0.84], [1.8]]), np.array([[0.8], [1.75]])]\n>>> \n>>> np.array([np.isclose(target.flatten(), output.flatten(), atol=1e-3) for target, output in zip(target_trajectory, trajectory)]).all()\nTrue",
         "failure_message": "Gradient Descent Trajectory Test Failed",
         "hidden": false,
         "locked": false,
         "points": 0,
         "success_message": "Gradient Descent Trajectory Test Passed"
        },
        {
         "code": ">>> c = np.array([[4], [5]])\n>>> dummy_fn = lambda x: float(c.T @ x)\n>>> dummy_fn_grad = lambda x: c.T\n>>> dummy_start_x, dummy_start_y = 1.0, 2.0\n>>> \n>>> trajectory, minimum_loc, minimum_value = gradient_descent(dummy_fn, dummy_fn_grad, start_x=dummy_start_x, start_y=dummy_start_y, lr=0.01, n_steps=5, silent=True)\n>>> target_minimum_loc = np.array([[0.8], [1.75]])\n>>> np.isclose(target_minimum_loc, minimum_loc, atol=1e-3).all()\nTrue",
         "failure_message": "Gradient Descent Minimum Location Test Failed",
         "hidden": false,
         "locked": false,
         "points": 0,
         "success_message": "Gradient Descent Minimum Location Test Passed"
        },
        {
         "code": ">>> c = np.array([[4], [5]])\n>>> dummy_fn = lambda x: float(c.T @ x)\n>>> dummy_fn_grad = lambda x: c.T\n>>> dummy_start_x, dummy_start_y = 1.0, 2.0\n>>> \n>>> trajectory, minimum_loc, minimum_value = gradient_descent(dummy_fn, dummy_fn_grad, start_x=dummy_start_x, start_y=dummy_start_y, lr=0.01, n_steps=5, silent=True)\n>>> target_minimum_value = 11.949999999999998\n>>> np.isclose(target_minimum_value, minimum_value, atol=1e-3)\nTrue",
         "failure_message": "Gradient Descent Minimum Value Test Failed",
         "hidden": false,
         "locked": false,
         "points": 0,
         "success_message": "Gradient Descent Minimum Value Test Passed"
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "Gradient Descent Initialization": {
     "name": "Gradient Descent Initialization",
     "points": 3,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> \n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "Q2.b.i Gradients of the Functions - f1": {
     "name": "Q2.b.i Gradients of the Functions - f1",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> B = np.array([[4, -2], [-2, 4]])\n>>> a = np.array([[0], [1]])\n>>> b = np.array([[-2], [1]])\n>>> dummy_input = np.array([[2.5], [3.5]])\n>>> output = f1_grad_exact(dummy_input)\n>>> \n>>> target_output = np.array([[3.0, 11.0]])\n>>> np.isclose(output, target_output, atol=1e-3).all()\nTrue",
         "failure_message": "Exact Gradients of f1 Test Failed",
         "hidden": false,
         "locked": false,
         "points": 0,
         "success_message": "Exact Gradients of f1 Test Passed"
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "Q2.b.ii Gradients of the Functions - f2": {
     "name": "Q2.b.ii Gradients of the Functions - f2",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> B = np.array([[4, -2], [-2, 4]])\n>>> a = np.array([[0], [1]])\n>>> b = np.array([[-2], [1]])\n>>> dummy_input = np.array([[2.5], [3.5]])\n>>> output = f2_grad_exact(dummy_input)\n>>> \n>>> target_output = np.array([[1.1858, 5.1032]])\n>>> np.isclose(output, target_output, atol=1e-3).all()\nTrue",
         "failure_message": "Exact Gradients of f2 Test Failed",
         "hidden": false,
         "locked": false,
         "points": 0,
         "success_message": "Exact Gradients of f2 Test Passed"
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "Q2.b.iii Gradients of the Functions - f3": {
     "name": "Q2.b.iii Gradients of the Functions - f3",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> B = np.array([[4, -2], [-2, 4]])\n>>> a = np.array([[0], [1]])\n>>> b = np.array([[-2], [1]])\n>>> dummy_input = np.array([[2.5], [3.5]])\n>>> output = f3_grad_exact(dummy_input)\n>>> \n>>> target_output = np.array([[0.02699379, 0.03779876]])\n>>> np.isclose(output, target_output, atol=1e-3).all()\nTrue",
         "failure_message": "Exact Gradients of f2 Test Failed",
         "hidden": false,
         "locked": false,
         "points": 0,
         "success_message": "Exact Gradients of f2 Test Passed"
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "Q4.a Squared Distance Function": {
     "name": "Q4.a Squared Distance Function",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> p = np.random.rand(4)\n>>> q = np.random.rand(4)\n>>> x = np.random.rand(4)\n>>> () == sq_dist_fwd(p, q, x).shape\nTrue",
         "failure_message": "Squared Distance Function Output Shape Test Failed",
         "hidden": false,
         "locked": false,
         "points": 0,
         "success_message": "Squared Distance Function Output Shape Test Passed"
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "Q4.c Squared Distance Function Gradients": {
     "name": "Q4.c Squared Distance Function Gradients",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> p = np.random.rand(4)\n>>> q = np.random.rand(4)\n>>> x = np.random.rand(4)\n>>> (4,) == sq_dist_grad(p,q,x).shape \nTrue",
         "failure_message": "Squared Distance Gradient Function Shape Test Failed",
         "hidden": false,
         "locked": false,
         "points": 0,
         "success_message": "Squared Distance Gradient Function Shape Test Passed"
        },
        {
         "code": ">>> #! think I prefer to test for us of \"grad\" from autograd, rather than do memory test\n>>> # 1. this is more robust ; 2. we can them provide demo of the memory use as proof of utility   \n>>> # \"\"\" # BEGIN TEST CONFIG\n>>> # name: test_sq_dist_grad_memory\n>>> # points: 2\n>>> # hidden: true \n>>> # success_message: 'Squared Distance Gradient Memory Test Passed'\n>>> # failure_message: 'Squared Distance Gradient Memory Test Failed'\n>>> # \"\"\" # END TEST CONFIG\n>>> # %%capture result\n>>> # %load_ext memory_profiler\n>>> # a,b,x = np.random.randn(100), np.random.randn(100), np.random.randn(100)\n>>> # %memit manual_grad_vec(a,b,x)\n>>> # %%capture result2\n>>> # %memit grad(sq_dist_fwd, 2)(a,b,x) \n>>> # get_mem_usage = lambda x: float(str(x)[12:str(x).find('MiB')])\n>>> # 3*get_mem_usage(result) < get_mem_usage(result2)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "Q4.d Squared Distance Autodiff": {
     "name": "Q4.d Squared Distance Autodiff",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> our_grad = grad(sq_dist, 0)(z) # This is calling our custom gradient function\n>>> their_grad = grad(sq_dist_fwd, 2)(p,q,z) # This is uses slow gradient primitives\n>>> np.allclose(our_grad, their_grad, atol=1e-3)\nTrue",
         "failure_message": "Squared Distance Autodiff Value Test Failed",
         "hidden": false,
         "locked": false,
         "points": 2,
         "success_message": "Squared Distance Autodiff Value Test Passed"
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  },
  "vscode": {
   "interpreter": {
    "hash": "3c89a9bb9c0cb0f9eadf088879d08118d7275393e9f46779256c83918de27aa5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
