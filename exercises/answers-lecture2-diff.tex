\section{Answers Lecture 2: Vector Differentiation}
\paragraph{\questionref{q:circle}} Answer discussed in lectures.

\paragraph{\questionref{q:index-notation}}

Matrix-vector expressions to index notation: 
\begin{enumerate}[label=\alph*.]
\item $\mat A \mat B \mat C \vx = \sum_{jkl} A_{ij} B_{jk} C_{kl} x_l$
\item $\Tr(\mat A) = \sum_i A_{ii}$
\item $\Tr(\mat A \mat B) = \sum_{ij} A_{ij} B_{ji}$
\item $\vy\transpose \mat A\transpose \vx = \sum_{ij} y_i A_{ji} x_j$
\end{enumerate}
Index notation to matrix-vector expressions:
\begin{enumerate}[label=\alph*.]
\item $\sum_{ijk} A_{ij}B_{jk}C_{ki} = \Tr(\mat A \mat B \mat C)$
\item $b_i + \sum_j A_{ij}b_j = \vb + \mat A \vb$
\item $x_ix_j = \vx \vx^T$
\item $\sum_j \delta_{ij}a_j = \va$
\end{enumerate}

\paragraph{\questionref{q:ind-not-proof}}

\begin{enumerate}
\item $\textbf{x}^TA\textbf{y} = \textbf{y}^TA\textbf{x}$ if $A=A^T$.

Note that $A=A^T \implies a_{ij}=a_{ij}$. We first write the product in terms of two vectors, $\textbf{x}$ and $A\textbf{y}$, then rearrange the terms considering $A\textbf{y}$ as a vector with components $(A\textbf{y})_i = \sum_{j=1}^N a_{ij}y_j$, and use $a_{ij} = a_{ji}$:
\begin{align*}
\textbf{x}^TA\textbf{y} = \sum_{i=1}^N x_i(A\textbf{y})_i = \sum_{i=1}^N x_i \sum_{j=1}^N a_{ij}y_j = \sum_{j=1}^N \sum_{i=1}^Ny_j a_{ji} x_i = \sum_{j=1}^N y_j (A\textbf{x})_j = \textbf{y}^TA\textbf{x}.
\end{align*}

\item $\textbf{x}^T\textbf{y} = Tr(\textbf{x}^T\textbf{y}) = Tr(\textbf{y}^T\textbf{x}), \textbf{x},\textbf{y}\in\mathbb{R}^D$.

Considering $\textbf{x}^T\textbf{y}\in\mathbb{R}$, we have $\textbf{x}^T\textbf{y}=Tr(\textbf{x}^T\textbf{y})$. Then, we need to check whether $\textbf{x}^T\textbf{y} = \textbf{y}^T\textbf{x}$, which we can show by rearranging the terms of the vector product
\begin{align*}
\textbf{x}^T\textbf{y} = \sum_{i=1}^N x_i y_i = \sum_{i=1}^N y_i x_i = \textbf{y}^T\textbf{x}.
\end{align*}

\item $Tr(ABC) = Tr(CAB)$. We assume $A\in\mathbb{R}^{D\times E}, B\in\mathbb{R}^{E\times F}$, and $C\in\mathbb{R}^{F\times D}.$

We start by inspecting the terms involving $Tr(ABC)$,
\begin{align*}
Tr(ABC) = \sum_{i=1}^D \big( (AB) C\big)_{ii} = \sum_{i=1}^D \sum_{j=1}^F (AB)_{ij} c_{ji} = \sum_{i=1}^D \sum_{j=1}^F \sum_{k=1}^E a_{ik} b_{kj} c_{ji}.
\end{align*}
Just by swapping the summations we can get the following identity: $Tr(ABC) = Tr(CAB) = Tr(BCA)$. We show $Tr(ABC) = Tr(CAB)$ as an example:
\begin{align*}
Tr(ABC) = \sum_{i=1}^D \sum_{j=1}^F \sum_{k=1}^E a_{ik} b_{kj} c_{ji} = \sum_{j=1}^F \sum_{k=1}^E \sum_{i=1}^D c_{ji}a_{ik}b_{kj} = \sum_{j=1}^F \sum_{k=1}^E (CA)_{jk}b_{kj} = \sum_{j=1}^F (CAB)_{jj} = Tr(CAB).
\end{align*}

\end{enumerate}

\paragraph{\questionref{q:mml55-56}}

\begin{enumerate}[label=\alph*]
    \item $f(\textbf{x}) = \sin(x_1)\cos(x_2), \quad \textbf{x}\in \mathbb{R}^2$
\[
\frac{\partial f}{\partial\textbf{x}} \in \mathbb{R}^{1\times 2}
\]
\begin{align*}
\frac{\partial f}{\partial\textbf{x}} &= \bigg[\frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2} \bigg]\\
&=\bigg[\cos(x_1)\cos(x_2), -\sin(x_1)\sin(x_2) \bigg]
\end{align*}
\item $f(\textbf{x}) = \textbf{x}^T\textbf{y}, \quad \textbf{x},\textbf{y}\in \mathbb{R}^n$
\[
\frac{\partial f}{\partial\textbf{x}} \in \mathbb{R}^{1\times n}
\]
We can solve this directly using basic rules of vector calculus
\[
\frac{\partial f}{\partial\textbf{x}} = \frac{\partial (\textbf{x}^T\textbf{y})}{\partial \textbf{x}} = \textbf{y}^T
\]
We can confirm this result holds with index notation. First, let us calculate the value $f(\textbf{x})$
\[
f(\textbf{x}) = \textbf{x}^T\textbf{y} = \sum_{i=1}^n x_iy_i
\]
\[
\frac{\partial f}{\partial x_j} = \frac{\partial}{\partial x_j}\sum_{i=1}^n x_iy_i =  \sum_{i=1}^n \frac{\partial x_j}{\partial x_i}y_i = \sum_{i=1}^n \delta_{ij}y_i = y_j
\]
\[
\frac{\partial f}{\partial\textbf{x}} = \left[\frac{\partial f}{\partial x_1}, \dots, \frac{\partial f}{\partial x_n}\right] = \left[y_1, \dots, y_n\right] = \textbf{y}^T
\]
\item $\textbf{f}(x) = \textbf{x}\textbf{x}^T, \quad \textbf{x}\in \mathbb{R}^n$
\[
\frac{\partial \textbf{f}}{\partial\textbf{x}} \in \mathbb{R}^{(n\times n)\times n}
\]
\[
\frac{\partial \textbf{f}}{\partial\textbf{x}}= C\quad \text{where $C$ is a 3D tensor.}
\]
\[
c_{ijk} = \frac{\partial f(\textbf{x})_{ij}}{\partial x_k}
\]
\[
\textbf{x}\textbf{x}^T = \begin{pmatrix}
    x_1   \\
    \vdots \\
    x_n \\
\end{pmatrix}
\begin{pmatrix}
    x_1 & \dots & x_n \\
\end{pmatrix} = \begin{pmatrix}
    x^2_1 & x_1 x_2 & \dots & x_1 x_n \\
    x_2 x_1 & x^2_2 &    & \vdots \\
    \vdots & & \ddots &  \vdots \\
    x_n x_1 & \dots & \dots & x^2_n \\
\end{pmatrix}
\]
\[
c_{ijk} = \frac{\partial (x_i x_j)}{\partial x_k} = \frac{\partial x_i}{\partial x_k} x_j + \frac{\partial x_j}{\partial x_k} x_i = \delta_{ik}x_j + \delta_{jk}x_i=  \begin{cases}
0 \quad &\text{if }k\neq i \text{ and } k\neq j\\
x_i \quad &\text{if }k=j \text{ and } i\neq j\\
x_j \quad &\text{if }k=i \text{ and } i\neq j\\
2x_i \quad &\text{if }k=i=j\\
\end{cases}
\]

    \item  $f(\textbf{t}) = \sin\big(\log(\textbf{t}^T\textbf{t})\big) \quad \textbf{t} \in \mathbb{R}^D$
    
We directly apply the chain rule
\[
\frac{\partial f}{\partial \textbf{t}} = \frac{\partial \sin\big(\log(\textbf{t}^T\textbf{t})\big)}{\partial \log(\textbf{t}^T\textbf{t})} \cdot \frac{\partial \log(\textbf{t}^T\textbf{t})}{\partial (\textbf{t}^T\textbf{t})} \cdot \frac{\partial (\textbf{t}^T\textbf{t})}{\partial \textbf{t}} 
\]
All of the terms are one dimensional except for $\frac{\partial (\textbf{t}^T\textbf{t})}{\partial \textbf{t}} \in\mathbb{R}^{1\times D}$. Let us calculate the value using the notation for vector calculus in the lectures. As in 5.5, we first calculate the value of $\textbf{t}^T\textbf{t}$ and its derivative w.r.t. $t_i$.
\[
\textbf{t}^T\textbf{t} = \sum_{i=1}^D t_i^2, \quad \frac{\partial (\textbf{t}^T\textbf{t})}{\partial t_i} = 2t_i
\]
\[
\frac{\partial (\textbf{t}^T\textbf{t})}{\partial \textbf{t}} = \left[\frac{\partial (\textbf{t}^T\textbf{t})}{\partial t_1}, \dots, \frac{\partial (\textbf{t}^T\textbf{t})}{\partial t_D}\right] = \left[2t_1 \dots, 2t_D\right] = 2\textbf{t}^T
\]

We can now use this result to proceed with the derivative of $f(\textbf{t})$.
\[
\frac{\partial f}{\partial \textbf{t}} = \cos \big(\log(\textbf{t}^T\textbf{t})\big) \cdot \frac{1}{\textbf{t}^T\textbf{t}} \cdot 2\textbf{t}^T
\]
\[
\frac{\partial f}{\partial \textbf{t}} = 2\textbf{t}^T \frac{\cos\big(\log(\textbf{t}^T\textbf{t})\big)}{\textbf{t}^T\textbf{t}}
\]

\item $f(X) = tr(AXB), \quad A\in \mathbb{R}^{D\times E},
X\in \mathbb{R}^{E\times F},
B\in \mathbb{R}^{F\times D}$

Use index notation:
\[
f(X) = tr(AXB) = \sum_{i=1}^D (AXB)_{ii}
\]
In order to fully compute $f(X)$, we need to calculate $(AXB)_{ii}$
\[
(AXB)_{ii} = \sum_{k=1}^F (AX)_{ik}b_{ki} = \sum_{k=1}^F \left(\sum_{l=1}^E a_{il}x_{lk}\right)b_{ki}
\]
Thus
\[
f(X) = \sum_{i=1}^D\sum_{k=1}^F \sum_{l=1}^E a_{il}x_{lk}b_{ki}
\]
Now we can just calculate the derivative using index notation
\[
\frac{\partial f}{\partial x_{nm}} = \frac{\partial}{\partial x_{nm}} \sum_{i=1}^D\sum_{k=1}^F \sum_{l=1}^E a_{il}x_{lk}b_{ki} = \sum_{i=1}^D\sum_{k=1}^F \sum_{l=1}^E a_{il}\frac{\partial x_{lk}}{\partial x_{nm}}b_{ki} = \sum_{i=1}^D\sum_{k=1}^F \sum_{l=1}^E a_{il}\delta_{ln}\delta_{km}b_{ki}
\]
Notice that in the last expression, all the terms in the summation cancel except when $k=m$ and $l=n$. Therefore
\[
\frac{\partial f}{\partial x_{nm}} = \sum_{i=1}^D a_{in}b_{mi} = \sum_{i=1}^D b_{mi}a_{in} = (BA)_{mn}
\]
Using this last result, we can calculate the derivative w.r.t. X.
\[
\frac{\partial f}{\partial X} = (BA)^T = A^TB^T
\]

Alternative proof: Use properties 4.19 and 5.100 from the MML book.
From 4.19
\[
f(X) = tr(AXB) = tr(XBA) = tr(XC), \quad C = BA
\]
and from 5.100
\[
\frac{\partial f}{\partial X} = \frac{\partial tr(XC)}{\partial X} = tr\left(\frac{\partial (XC)}{\partial X}\right), \quad \text{where } \frac{\partial (XC)}{\partial X}\in \mathbb{R}^{(E \times E) \times (E \times F)}
\]

We need to calculate $\frac{\partial (XC)_{ij}}{\partial x_{kl}}$, and we find convenient to write the pairs $i,j$ of the product $IXC$, where $I \in \mathbb{R}^{E\times E}$ is the identity matrix.
\[
(IXC)_{ij} = \sum_{e=1}^E \sum_{f=1}^F \delta_{ie}x_{ef}c_{fj}
\]
\[
\frac{\partial (XC)_{ij}}{\partial x_{kl}} = \frac{\partial (IXC)_{ij}}{\partial x_{kl}} = \delta_{ik}c_{lj}
\]
in the prevous expression, all the terms in the sum vanish except the ones that contain $x_{kl}$ in it.

Now, we take into account the definition of the trace for any 4D tensor $T\in\mathbb{R}^{(N\times N)\times (P \times Q)}$ given in the MML book:
\[
tr(T)_{ij} = \sum_{k=1}^N a_{kkij}, \quad \text{where } tr(T) \in \mathbb{R}^{P\times Q}
\]

We use this definition to calculate our result.
\[
tr\left(\frac{\partial (XC)}{\partial X}\right)_{ij} = \sum_{k=1}^{E} \frac{\partial (XC)_{kk}}{\partial x_{ij}} = \sum_{k=1}^{E} \delta_{ki} c_{jk} = c_{ji}
\]
all the terms will be 0 except when $k=i$.
\[
tr\left(\frac{\partial (XC)}{\partial X}\right) = C^T = (BA)^T = A^TB^T
\]
\end{enumerate}


\paragraph{\questionref{q:chain-rule}}

\begin{enumerate}[label=\alph*.]
    \item $f(z) = \log(1 + z), \quad z = \textbf{x}^T\textbf{x}, \quad \textbf{x}\in\mathbb{R}^D$
    
\[
\frac{\partial f}{\partial \textbf{x}} = \frac{\partial f}{\partial z} \frac{\partial z}{\partial \textbf{x}} = \frac{\partial \log(1 + z)}{\partial z} \frac{\partial (\textbf{x}^T\textbf{x})}{\partial \textbf{x}} = \frac{2\textbf{x}^T}{1 + z} = \frac{2\textbf{x}^T}{1 + \textbf{x}^T\textbf{x}} 
\]
Dimensions are 
\[
\frac{\partial f}{\partial \textbf{x}} \in \mathbb{R}^D, \quad \frac{\partial f}{\partial z} \in \mathbb{R}, \quad \frac{\partial z}{\partial \textbf{x}} \in \mathbb{R}^D
\]
    
    \item $f(\textbf{z}) = \sin(\textbf{z}), \quad \textbf{z} = A\textbf{x} + \textbf{b}, \quad A\in\mathbb{R}^{E\times D}, \textbf{x}\in\mathbb{R}^D, \textbf{b}\in\mathbb{R}^E$

\[
\frac{\partial f}{\partial \textbf{x}} = \frac{\partial f}{\partial \textbf{z}} \frac{\partial \textbf{z}}{\partial \textbf{x}} = \frac{\partial \sin(\textbf{z})}{\partial \textbf{z}} \frac{\partial (A\textbf{x} + \textbf{b})}{\partial \textbf{x}}
\]
Notice that $\frac{\partial f}{\partial \textbf{z}} \in \mathbb{R}^{E\times E}$. We already know that $\sin(\cdot)$ is applied to each element independently, thus
\[
\frac{\partial f_i}{\partial z_j}=\begin{cases}
0 \quad &\text{if }i\neq j\\
\cos(z_i) \quad &\text{if }i=j\\
\end{cases}
\]
We also have $\frac{\partial \textbf{z}}{\partial \textbf{x}} \in \mathbb{R}^{E\times D}$. Although this has already shown in the lectures, let us review the result $\frac{\partial \textbf{z}}{\partial \textbf{x}}$ using the notation of the course.
\[
z_i = \sum_{j=1}a_{ij}x_j + b_i
\]
We can now easily compute $\frac{\partial z_i}{\partial x_j}$
\[
\frac{\partial z_i}{\partial x_j} = a_{ij}, \quad \frac{\partial \textbf{z}}{\partial \textbf{x}} = A
\]
Let us use all the previous results to compute the derivative of $f(\textbf{z})$ w.r.t. $\textbf{x}$.
\[
\frac{\partial f}{\partial \textbf{x}}= diag(\cos(\textbf{z})) A, \quad \text{where e.g. } diag(\textbf{v}) = \begin{pmatrix}
    v_1 & 0 & \dots & 0 \\
    0 & v_2 & \dots & 0 \\
    \vdots & & \ddots & \vdots \\
    0 & \dots & \dots & v_N \\
\end{pmatrix},\quad \textbf{v}\in\mathbb{R}^N
\]
\[
\frac{\partial f}{\partial \textbf{x}}= diag(\cos(A\textbf{x} + \textbf{b}))A
\]
Dimensions are 
\[
\frac{\partial f}{\partial \textbf{x}} \in \mathbb{R}^{E\times D}, \quad \frac{\partial f}{\partial \textbf{z}} \in \mathbb{R}^{E\times E}, \quad \frac{\partial \textbf{z}}{\partial \textbf{x}} \in \mathbb{R}^{E\times D}
\]

    \item $f(z) = \exp(-\frac{1}{2}z), \quad z = g(\textbf{y}) =  \textbf{y}^TS^{-1}\textbf{y}, \quad \textbf{y} = h(\textbf{x}) = \textbf{x} - \bm{\mu}, \quad \textbf{x},\bm{\mu}\in\mathbb{R}^D, S\in\mathbb{R}^{D\times D}$
    
\begin{align*}
\frac{\partial f}{\partial \textbf{x}} &= \frac{\partial f}{\partial z} \frac{\partial z}{\partial \textbf{y}}\frac{\partial \textbf{y}}{\partial \textbf{x}} = \frac{\partial \exp(-\frac{1}{2}z)}{\partial z} \frac{\partial (\textbf{y}^TS^{-1}\textbf{y})}{\partial \textbf{y}}\frac{\partial (\textbf{x} - \bm{\mu})}{\partial \textbf{x}} = \exp\left(-\frac{1}{2}z\right)\left(-\frac{1}{2}\right)\textbf{y}^T(S^T + S^{-T})I \\
&=-\frac{1}{2}\exp\left(-\frac{1}{2}\Big((\textbf{x} - \bm{\mu})^TS^{-1}(\textbf{x} - \bm{\mu})\Big)\right)(\textbf{x} - \bm{\mu})^T(S^T + S^{-T})
\end{align*}
where $S^{-T} = \left(S^{-1}\right)^T$, and we use (5.107) to calculate $\frac{\partial (\textbf{y}^TS^{-1}\textbf{y})}{\partial \textbf{y}}$.

Dimensions are 
\[
\frac{\partial f}{\partial \textbf{x}} \in \mathbb{R}^D, \quad \frac{\partial f}{\partial z}\in\mathbb{R}, \quad \frac{\partial z}{\partial \textbf{y}}\in\mathbb{R}^{D}, \quad \frac{\partial \textbf{y}}{\partial \textbf{x}}\in\mathbb{R}^{D\times D}
\]
    
    \item $f(\textbf{x}) = tr(\textbf{x}\textbf{x}^T + \sigma^2I),\quad\textbf{x}\in\mathbb{R}^D$
    
Let us expand $f(x)$.
\begin{align*}
f(x)&= \sum_{i=1}^D \Big( (\textbf{x}\textbf{x}^T)_{ii} + \sigma^2 \Big)\\
    &= \sum_{i=1}^D (\textbf{x}\textbf{x}^T)_{ii} + D\sigma^2 = \sum_{i=1}^D x_i^2 + D\sigma^2
\end{align*} 
We already know that $(\textbf{x}\textbf{x}^T)_{ij} = x_ix_j$. Therefore
\[
\frac{\partial f}{\partial \textbf{x}} = \frac{\partial \left( \sum_{i=1}^D x_i^2 + D\sigma^2 \right)}{\partial \textbf{x}} = 2\textbf{x}^T
\]

    \item $f(\textbf{z}) = \tanh(\textbf{z})\in\mathbb{R}^M,\quad \textbf{z}= A\textbf{x} + \textbf{b}, \quad \textbf{x}\in\mathbb{R}^N, A\in\mathbb{R}^{M\times N}, \textbf{b}\in\mathbb{R}^M$
\begin{align*}
\frac{\partial f}{\partial \textbf{x}} &= \frac{\partial f}{\partial \textbf{z}} \frac{\partial \textbf{z}}{\partial \textbf{x}}= \frac{\partial \tanh(\textbf{z})}{\partial \textbf{z}} \frac{\partial (A\textbf{x} + \textbf{b})}{\partial \textbf{x}} = diag\left(1 - \tanh^2(\textbf{z})\right) \textbf{A}\\
&= diag\left(1 - \tanh^2(A\textbf{x} + \textbf{b})\right) A 
\end{align*}
where we used $\frac{d \tanh(v)}{dv} = 1 - \tanh^2(v)$.

Dimensions are 
\[
\frac{\partial f}{\partial \textbf{x}} \in \mathbb{R}^{M\times N}, \quad \frac{\partial f}{\partial \textbf{z}}\in\mathbb{R}^{M\times M}, \quad \frac{\partial z}{\partial \textbf{x}}\in\mathbb{R}^{M\times N}
\]

\item $f(A) = \textbf{x}\transpose A \textbf{x}, \quad A=\textbf{x}\textbf{x}\transpose, \quad A\in\mathbb{R}^{N\times N}$

Note that $A$ is symmetric. We apply the chain rule straightforwardly.
\begin{align*}
\frac{d f}{d \textbf{x}} &= \frac{\partial f}{\partial \textbf{x}} + \frac{\partial f}{\partial A} \frac{\partial A}{\partial \textbf{x}} = \frac{\partial (\textbf{x}^T A \textbf{x}) }{\partial \textbf{x}} + \sum_{i,j}\frac{\partial (\textbf{x}^T A \textbf{x}) }{\partial a_{ij}} \frac{\partial a_{ij}}{\partial\textbf{x}}
\end{align*}

The first term can be computed using vector calculus rules
\begin{align*}
\frac{\partial (\textbf{x}^T A \textbf{x}) }{\partial \textbf{x}}  = 2A\textbf{x} = 2\textbf{x}\textbf{x}^T\textbf{x} = 2 ||\textbf{x}||^2\textbf{x}.
\end{align*}
We can compute the second term using index notation, i.e. the derivative with respect to $x_k$.
\begin{align*}
\sum_{i,j}\frac{\partial (\textbf{x}^T A \textbf{x}) }{\partial a_{ij}} \frac{\partial a_{ij}}{\partial x_k} &= \sum_{i,j} a_{ij}\left(\delta_{ik}x_j + \delta_{jk}x_i\right) = \sum_{i,j} a_{ij}\delta_{ik}x_j + \sum_{i,j} a_{ij}\delta_{jk}x_i\\
&= \sum_{j} a_{kj}x_j + \sum_{i} a_{ik}x_i = 2 \sum_{i} x_k x_i^2 = 2 ||\textbf{x}||^2x_k.
\end{align*}
Where we used vector calculus rules and \questionref{q:mml55-56} c) to derive the following
\begin{align*}
\frac{\partial (\textbf{x}^T A \textbf{x}) }{\partial A}  = \textbf{x}\textbf{x}^T = A, \quad \frac{\partial a_{ij}}{\partial x_k} = \delta_{ik}x_j + \delta_{jk}x_i, \quad a_{ij} = x_ix_j.
\end{align*}
In conclusion, we have
\begin{align*}
\frac{d f}{d \textbf{x}} &= \frac{\partial (\textbf{x}^T A \textbf{x}) }{\partial \textbf{x}} + \sum_{i,j}\left(\frac{\partial (\textbf{x}^T A \textbf{x}) }{\partial a_{ij}}\right) \frac{\partial a_{ij}}{\partial\textbf{x}} = 2 ||\textbf{x}||^2\textbf{x}+ 2 ||\textbf{x}||^2\textbf{x} = 4 ||\textbf{x}||^2\textbf{x}.
\end{align*}

Extra: one can check the previous result is true by simplifying the initial form of $f$, which we maintained for illustrative purposes, i.e. $f(\textbf{x}) = ||\textbf{x}||^4$. We can compute the derivative using index notation
\begin{align*}
\frac{df}{d x_i} = \frac{d}{d x_i}\left(\sum_{i=1}^N x_i^2 \right)^2 = 2||\textbf{x}||^2 2x_i = 4||\textbf{x}||^2 x_i,
\end{align*}
which in vector form is expressed as follows
\begin{align*}
\frac{df}{d x} = 4||\textbf{x}||^2 \textbf{x}.
\end{align*}

\end{enumerate}


\paragraph{\questionref{q:hessian}}
The objective function and gradient w.r.t.~$\vtheta$ (see lectures) for Linear Regression is
\begin{align}
L(\vtheta) = \norm{\vy - \Phi(X)\vtheta}^2 \,, &&
\deriv[L]{\vtheta} = 2(\Phi(X)\vtheta - \vy)\transpose\Phi(X) \,.
\end{align}
We begin by finding the Hessian, i.e.~the matrix containing all second partial derivatives. We need to do this in index notation, as the vector conventions of our vector chain rule break down. So we first write the derivative in index notation, and then we take the derivative again, after which we return to vector notation:
\begin{align}
\pderiv[]{\theta_j} \left(\pderiv[L]{\theta_i}\right) &=
\pderiv[]{\theta_j} \left(2\sum_k\left(\sum_m\Phi_{km}\theta_m - y_k\right)\Phi_{ki}\right) =
\pderiv[]{\theta_j} \left(2\sum_k\left(\sum_m\Phi_{km}\theta_m - y_k\right)\Phi_{ki}\right) \\
&= 2\sum_{km}\Phi_{km}\delta_{mj}\Phi_{ki} = 2\sum_{k}\Phi_{kj}\Phi_{ki} \,, \\
\implies \mat H_{\vtheta}(L) &= 2 \Phi(X)\transpose\Phi(X) \,.
\end{align}
The Hessian doesn't depend on the parameter $\vtheta$, so if we prove that the matrix is positive definite, then the local where $\deriv[L]{\theta} = 0$ (see lecture slides) will be a minimum. For a matrix to be PD, we need $\vv\transpose\mat H\vv > 0$ for all $\vv$. We substitute our Hessian into $\mat H$ to prove this
\begin{align}
\vv\transpose \mat H \vv &= 2\vv\transpose \Phi(X)\transpose\Phi(X) \vv \\
&= \vw\transpose\vw = \sum_i w_i^2\,, && \text{with } \vv = \Phi(X)\vv\,.
\end{align}
This already shows that $\vv\transpose \mat H \vv \geq 0$, with equality if there exists a $\vv$ such that $\Phi(X)\vv = 0$. So now we need to prove that \emph{there cannot be} a $\vv$ for which $\Phi(X)\vv = 0$. If $\mathrm{rank}\,\Phi(X) \geq M$, then this will not happen, by the rank-nullity theorem \citep[\S 2.7.3]{mml}.

At this point, we need to assume this is the case. For full marks though, you should state the implications on the problem at hand, rather than in abstract maths. One \emph{necessary} implication of this is that $N\geq M$. This is only a necessary condition, rather than a sufficient one, since even of $N \geq M$, $\Phi(X)$ can still have many linearly dependent rows. This will at least happen if you observe repeated input points. However, to prove more than this, you need more information about $\Phi(X)$.\footnote{A case that is harder think about is if you observe points that make the feature vectors $\vphi(\vx_n)$ linearly dependent. One example is if you have a 2D input with $\vphi(\vx) = \vx\transpose$, and all your input points lie on a line.}

So to summarise, we could prove that \textbf{if $\mathrm{rank}\,, \Phi(X) \geq M$, which at least needs $N\geq M$, then Linear Regression has a single minimum solution}.

If we are coding up a linear regression problem, and we want to check numerically for a \emph{specific} regression problem whether there is a unique solution, we can compute the eigenvalues of $\Phi(X)\transpose\Phi(X)$, and see if they are all positive. This implies a PD Hessian because
\begin{align}
\vv\transpose \mat H \vv &= \vv\transpose \mat Q \mat \Lambda \mat Q\inv \vv && \text{(eigenvalue decomposition)} \\
&= \vv\transpose \mat Q \mat \Lambda \mat Q\transpose\vv && \text{($\mat H = \mat H\transpose$, so $\mat Q\mat \Lambda \mat Q\inv = (\mat Q\mat \Lambda \mat Q\inv)\transpose$, so $\mat Q\inv = Q\transpose$)}\\
&= \vz\transpose \Lambda \vz \,,
\end{align}
which is only $> 0$ if all the elements in the diagonal matrix $\Lambda$ are positive.

If any of the linear algebra was unfamiliar, I recommend looking at chapter 2 in \citet{mml}, particularly \S2.3, \S2.6, and \S2.7, or the 1st year linear algebra course.


\section{Answers Lecture 3: Automatic Differentiation}



\subsection{\questionref{q:autodiff-productrule}}
We begin by drawing the computational graph (\cref{fig:qproductrule-compgraph}). % We make the distinction between $v_i$ and the functions to highlight the difference between an \emph{evaluation} of a function, and the function itself. All the $v_i$s refer to specific evaluations, and are therefore numbers.
We now find the primal trace and the forward tangent trace:
\begin{align}
v_0 &= x && \pderiv[v_0]{x} = 1 \\
v_1 &= a(x) && \pderiv[v_1]{x} = \pderiv[v_1]{v_0}\pderiv[v_0]{x} = \pderiv[a(x)]{x} \\
v_2 &= b(x) && \pderiv[v_2]{x} = \pderiv[v_2]{v_0}\pderiv[v_0]{x} = \pderiv[b(x)]{x} \\
v_3 &= v_1 \cdot v_2 && \pderiv[v_3]{x} = \sum_{j \in \mathrm{inputs}(3)}\pderiv[v_3]{v_j}\pderiv[v_j]{x} = v_2\pderiv[a(x)]{x} + v_1\pderiv[b(x)]{x} \,.
\end{align}
This means that for any x, forward mode autodiff calculates the derivative to be:
\begin{align}
\deriv[f]{x} = b(x)\deriv[a(x)]{x} + a(x)\deriv[b(x)]{x} \,.
\end{align}
Which is the product rule.

If we substitute $a(x) = x, b(x) = x$, then we obtain $\calcd f / \calcd x = 2x$, as expected.


\begin{figure}[t]
\centering
  \tikz{
 \node[const] (x) {$x$};%
 \node[latent, right=of x] (v0) {$v_0$};%
 \node[latent, right=of v0, yshift=0.6125cm] (v1) {$v_1$};
 \node[latent, right=of v0, yshift=-0.6125cm] (v2) {$v_2$};
 \node[latent, right=of v1, yshift=-0.6125cm] (v3) {$v_3$};
 \node[const, right=of v3] (out) {out};
 \edge {x}  {v0};
 \edge {v0} {v1,v2};
 \edge {v1,v2}  {v3};
 \edge {v3}     {out};
}
\caption{Computational graph for \questionref{q:autodiff-productrule}.}
\label{fig:qproductrule-compgraph}
\end{figure}



\subsection{\questionref{q:autodiff}}
This is a rather big question. It is designed to test \emph{every} single differentiation skill we taught, and how it fits together. As such, I think it's great preparation for the exam. However, it is long. You may want to try to do parts of it yourself, and perhaps part in a group.

\paragraph{Part a)}
\begin{align}
D_a &: \Reals^{N\times 2} &&\to \Reals^{N\times N} \\
\Lambda_a &: \Reals^{N\times N}\times \Reals^{2} &&\to \Reals^{N\times N} \\
K_a &: \Reals^{N\times N} &&\to \Reals^{N\times N} \\
f &: \Reals^{N\times N}\times\Reals^{N\times N} &&\to \Reals
\end{align}

\paragraph{Part b)} See \cref{fig:qautodiff}. We use names evident from the question for some nodes, but give new names to some additional intermediate notes. How should one decide to split up the computational graph? You should at least split according to the sequence of functions given in the question. Hence why we have the nodes named the same as intermediate functions. We should also split up depending on for which operations gradients are defined in our autodiff framework. So for example, in the computation of $f$, we need to compute a matrix sum, matrix inverse, and then a vector quadratic. We know how to differentiate each one of these operations separately, so we split them up so we can chain them together. For the vector quadratic, we have a choice. We could see this as a single operation, or we could split this into two matrix-vector multiplications. Autodiff frameworks probably do the latter, but for this question, we consider it done together. In an exam situation, the split will be defined clearly for you, or you would explicitly be asked to state your assumptions when making splits.
\begin{figure}[h]
\centering
  \tikz{
 \node[const] (l) {$\boldsymbol\ell$};%
 \node[const,  below=of l] (X) {$\mat X$};%
 \node[latent, right=of X] (v0) {$v_0$};%
 \node[latent, right=of v0] (D1) {$D_1$};
 \node[latent, right=of v0, yshift=-1.25cm] (D2) {$D_2$};
 \node[latent, right=of D1, yshift=0.6125cm] (L1) {$\Lambda_1$};
 \node[latent, right=of D2, yshift=0.6125cm] (L2) {$\Lambda_2$};
 \node[latent, above=of D1,yshift=-0.4333cm] (vn1) {$v_{-1}$};%
 \node[latent, right=of L1] (K1) {$K_1$};
 \node[latent, right=of L2] (K2) {$K_2$};
 \node[latent, right=of K2, yshift=0.6125cm] (K)  {$v_1$};
 \node[latent, right=of K] (Kinv)  {$v_2$};
 \node[latent, right=of Kinv] (v3)  {$v_3$};
 \node[const, right=of v3] (out)  {out};
 % \node[latent, right=of v0, yshift=0.6125cm] (v1) {$v_1$};
 % \node[latent, right=of v0, yshift=-0.6125cm] (v2) {$v_2$};
 % \node[latent, right=of v1, yshift=-0.6125cm] (v3) {$v_3$};
 % \node[const, right=of v3] (out) {out};
 \edge {l}  {vn1};
 \edge {X}  {v0};
 \edge {v0} {D1};
 \edge {v0}  {D2};
 \edge {D1} {L1};
 \edge {D2} {L2}; 
 \edge {vn1} {L1,L2};
 \edge {L1} {K1};
 \edge {L2} {K2}; 
 \edge {K1, K2} {K};
 \edge {K} {Kinv};
 \edge {Kinv} {v3};
 \edge {v3} {out};
}
\caption{Computational graph for \questionref{q:autodiff}, where we define $v_1 = \mat K_1 + \mat K_2$, $v_2 = v_1\inv$, and $v_3 = \vy\transpose v_1 \vy$.}
\label{fig:qautodiff}
\end{figure}

\paragraph{Part c)} Let's first consider \textbf{forward mode}. For the derivatives w.r.t.~$\mat X$, we initialise as:
\begin{align}
&\text{Primal} && \text{Forward tangent} && \text{Notes} \nonumber \\
v_{-1} &= \boldsymbol \ell && \dot v_{-1,iab} = \pderiv[{[v_{-1}]}_{i}]{X_{ab}} = 0 && v_{-1} \in \Reals^{2 \times (N\times 2)}\,, O(N) \\ % \pderiv[v_{-1}]{\boldsymbol\ell} = \mat I_2
v_0 &= X && \dot v_0 = \pderiv[{[v_0]}_{ij}]{X_{ab}} = \delta_{ia}\delta_{jb}  &&  \dot v_0 \in \Reals^{(N\times 2)\times (N\times 2)}\,, O(N^2)
\end{align}
While for the derivatives w.r.t.~$\boldsymbol \ell$, we initialise as:
\begin{align}
&\text{Primal} && \text{Forward tangent} && \text{Notes} \nonumber \\
v_{-1} &= \boldsymbol \ell && \dot v_{-1,ia} = \pderiv[{[v_{-1}]}_{i}]{\ell_{a}} = \delta_{ai} && v_{-1} \in \Reals^{2 \times 2}\,, O(1) \\ % \pderiv[v_{-1}]{\boldsymbol\ell} = \mat I_2
v_0 &= X && \dot v_0 = \pderiv[{[v_0]}_{ij}]{\ell_{a}} = 0  &&  \dot v_0 \in \Reals^{(N\times 2)\times 2}\,, O(N) \\
\end{align}
Then we follow the forward mode computations. In the following calculation, we consider the gradient of $\mat X$, so all forward tangents will have indices $a$ and $b$ corresponding to values in $\mat X$. The computations w.r.t.~$\boldsymbol \ell$ are exactly the same, but without the index $b$.
\begin{align}
&\text{Primal} && \text{Forward tangent} && \text{Notes} \nonumber \\
D_z &= \dots && \pderiv[{[D_z]}_{nm}]{v_{0ij}} = \pderiv{v_{0ij}} (v_{0nz} - v_{0mz})^2 &&  \dot D_z \in \Reals^{(N\times N) \times (N\times 2)}  \nonumber \\
& && \qquad = 2(v_{0nz} - v_{0mz})(\delta_{ni}\delta_{zj} - \delta_{mi}\delta_{zj}) &&\\
& && \dot D_{znmab} = \left[\pderiv[D_z]{v_0} \dot v_0\right]_{nmab} && O(N) \text{ for sum, so total } O(N^4). \\
& && \quad = 2 (v_{0nz} - v_{0mz}) (\dot v_{0nzab} - \dot v_{0mzab})&& \text{Structure allows } O(N^3). \\
\Lambda_z &= - \frac{D_z}{2v_{-1z}^2} && \pderiv[\Lambda_{zij}]{D_{znm}} = - \frac{1}{2v_{-1z}^2}\delta_{in}\delta_{jm}  && \dot \Lambda_z \in \Reals^{(N\times N) \times (N\times 2)} \\
& && \pderiv[\Lambda_{zij}]{v_{-1,k}} = \frac{D_{zij}}{v_{-1z}^3} \delta_{zk} && \\
& && \dot \Lambda_{zijab} = \left[\pderiv[\Lambda_{z}]{D_{z}} \dot D_{z} + \pderiv[\Lambda_{z}]{v_{-1}} \dot v_{-1}\right]_{ijab} && O(N^2)\text{ for sum, so total }O(N^5)\,. \\
& && \quad = - \frac{1}{2v_{-1z}^2} \dot D_{zijab} + \frac{D_{zij}}{v_{-1z}^3} \dot v_{-1,zab} && \text{Structure allows } O(N^3)\,. \\
K_z &= \exp(\Lambda_z) && \pderiv[K_{znm}]{\Lambda_{zij}} = \exp(\Lambda_{zij})\delta_{ni}\delta_{mj} && \dot K_z \in \Reals^{(N\times N)\times (N\times 2)} \\
& && \dot K_{znmab} = \exp(\Lambda_{znm})\dot\Lambda_{znmab} && \text{Structure allows } O(N^3)\,. \\
v_1 &= \mat K_1 + \mat K_2 && \pderiv[v_{1ij}]{K_{znm}} = \delta_{in}\delta_{jm} && \\
& && \dot v_1 = \pderiv[v_1]{\mat K_1} \dot{\mat K}_1 + \pderiv[v_1]{\mat K_2} \dot{\mat K}_2 && \\
& && \dot v_{1,ijab} =  \dot K_{1,ijab} + \dot K_{z,ijab} && \\
v_2&= v_1\inv && \pderiv[v_2]{v_1} = -v_1\inv \pderiv[v_1]{v_1}v_1\inv && \text{This is an identity.} \\
& && \pderiv[v_{2,nm}]{v_{1,ij}} = \sum_{pq}-v_{1,np}\inv \delta_{pi}\delta_{qj} v_{1,qm}\inv && A_{pq}\inv = [A\inv]_{pq} \\
& && \dot v_{2,nmab} = \sum_{pqij} -v_{1,np}\inv \delta_{pi}\delta_{qj} v_{1,qm}\inv \dot v_{1,ijab} && \\
& && \quad = -\sum_{p} v_{1,np} \sum_q \dot v_{1,pqab} v_{1,qm} && O(N^4) \\
v_3 &= \vy\transpose v_2 \vy && \pderiv[v_3]{v_{2,nm}} = y_ny_m  && \\
& &&\dot v_{3,ab} = \sum_{nm} y_ny_m \dot v_{2,nmab} && \text{Done!}
\end{align}

The bottleneck computational cost we see is $O(N^4)$. Let's compare this to the cost of computing the objective function. We see a few steps that are $O(N^2)$ (like computing the distances), and the matrix inverse, which is $O(N^3)$. We know that forward mode gives us a guarantee of computing the gradient that is linear in the number of inputs, multiplied by the cost of evaluating the function. Since we have $O(N)$ inputs, we see that the $O(N^4)$ bottleneck is consistent with this!

The bottleneck comes from the identity for differentiating an inverse. If we would have differentiated through the actual algorithm that computes the inverse, rather than using the identity, we would have obtained the same computational cost.

Let's next consider \textbf{reverse mode}. We use the bar notation to denote the derivative w.r.t.~the input of a node, i.e.~$\bar v_3 = \pderiv[f]{v_2}$. The primal mode column is executed top to bottom, after which the reverse adjoint column is executed bottom to top.

\begin{align}
&\text{Primal} && \text{Reverse adjoint} && \text{Notes} \nonumber \\
D_z &= \dots && \bar v_{0,ij} = 2\left(\sum_{m} \bar D_{jim} (v_{0ij} - v_{0mj}) - \sum_{n} \bar D_{jni} (v_{0nj} - v_{0,ij})\right) && \text{Done! } O(N^2) \\
& && \pderiv[D_{znm}]{v_{0ij}} = 2(v_{0nz} - v_{0mz})(\delta_{ni}\delta_{zj} - \delta_{mi}\delta_{zj}) &&\\
\Lambda_z &= - \frac{D_z}{2v_{-1z}^2} && \bar D_{znm} = -\bar \Lambda_{znm} \frac{D_{znm}}{2v_{-1z}^2} && \\
& && \bar v_{-1,z} = \sum_{ij} \bar \Lambda_{zij} \frac{D_{zij}}{v_{-1,z}^3}  && \text{Done! } O(N^2)  \\
& && \pderiv[\Lambda_{zij}]{D_{znm}} = -\frac{D_{znm}}{2v_{-1z}^2}\delta_{in}\delta_{jm}  && \\
& && \pderiv[\Lambda_{zij}]{v_{-1,k}} = \frac{D_{zij}}{v_{-1z}^3} \delta_{zk} && \\
K_z &= \exp(\Lambda_z) && \bar \Lambda_{zij} = \bar K_{zij} K_{zij} && \\
& && \pderiv[K_{znm}]{\Lambda_{zij}} = \exp(\Lambda_{zij})\delta_{ni}\delta_{mj} && \\
v_1 &= \mat K_1 + \mat K_2 && \bar K_{znm} = \sum_{ij} \bar v_{1,ij} \delta_{in}\delta_{jm} = \bar v_{1,nm} && \\
& && \pderiv[v_{1ij}]{K_{znm}} = \delta_{in}\delta_{jm} && \\
v_2&= v_1\inv && \bar v_{1,ij} = \sum_{nm} \bar v_{2,nm} \pderiv[v_{2,nm}]{v_{1,ij}} = -\sum_{n} v_{1,ni}\inv \sum_m \bar v_{2,nm} v_{1,jm}\inv && O(N^3) \\
& && \pderiv[v_{2,nm}]{v_{1,ij}} = -v_{1,ni}\inv v_{1,jm}\inv && \text{See forward mode.} \\
v_3 &= \vy\transpose v_2 \vy && \bar v_{2,nm} = \bar v_3 y_my_n && \\
& && \bar v_2 = \bar v_3\pderiv[v_3]{v_{2}} && \\
& && \pderiv[v_3]{v_{2,nm}} = y_ny_m  && \\
\text{out} &= v_3 && \bar v_3 = 1 &&
\end{align}

The computational complexity this way round is cheaper that for the forward mode, since we have many variables that we are differentiating with respect to. The cost for each reverse step is the same as the forward step.



%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "exercises"
%%% End: 
